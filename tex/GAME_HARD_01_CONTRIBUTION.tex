% GAME_HARD_01_CONTRIBUTION.tex

% CONTRIBUTION
\section{CONTRIBUTION}
\label{sec:contribution}

% CONTRIBUTION - OBJECT OF STUDY
\subsection{Object of Study}
\label{sec:contribution:objectofstudy}
Due to the nature of the study, it is important that the experiment process is measurable and deterministic.
For the purpose of the experiment, the data computed corresponds to a square matrix multiplication, since such an operation is highly parallelizable, as demonstrated by Kirk~and~Hwu~\cite[ch.~3]{Kirk:2010:PMP:1841511}.
Additionally, verifying the result of such an operation is trivial, and potential losses in computational precision may be easily established.
The operation is therefore considered suitable for the purpose of the study.
Furthermore, the matrix dimension $200\times 200$ is selected for the respective matrices as it is big enough to execute efficiently on the GPU, but yet reasonably sized to keep simulated measurements comparable to those of the hardware accelerated reference case.
Throughout this material, the compiled data is referred to as $AB=C$, and thus make out the Object of this experiment.

% CONTRIBUTION - METHOD OF STUDY
\subsection{Method of Study}
\label{sec:contribution:methodofstudy}
In order to establish the Object~of~Study, $AB=C$, the experiment utilizes DirectCompute HLSL kernels using which the result matrix is calculated, in its entirety, using some device driver, or Subject (see section \nameref{sec:contribution:subjectsofstudy}).
The experiment is devised of the following approximate steps in order to compile the Object~of~Study $AB=C$:
\begin{enumerate*}
	\item Randomize two matrices $A$ and $B$ of desired data-type (single floating point or integer).
	\item Establish the product-matrix $AB=Ref$. The resulting matrix will be used as a reference matrix to verify the final result.
	\item Start a synchronized high-precision timer.
	\item Dispatch the DirectCompute program calculating the product matrix $AB=C$.
	\item Stop the timer once the kernel has finished execution.
	\item Establish possible deviation between resulting matrix $C$ and previously established matrix $Ref$.
\end{enumerate*}
The above process  makes out the Method of this study.

% CONTRIBUTION - SUBJECTS OF STUDY
\subsection{Subjects of Study}
\label{sec:contribution:subjectsofstudy}
The kernels described in section \nameref{sec:contribution:methodofstudy} are comprised of HLSL, compiled using Microsoft FXC, and executed using DirectCompute.
The programs are run using three types of acceleration technologies with varying strategies of simulation.
These are comprised of the following:

\subsubsection{Hardware-Acceleration (GPU)}
The execution of a DirectCompute-kernel on a graphics card.
By utilizing a modern video card, hardware acceleration allows DirectX to utilize the throughput-oriented design of modern GPUs; maximizing FLOPS (FLoating-point Operations Per Second) capacity.
Being the common case; the methodology involves no simulation.
Thus, measurements collected using this device driver acts as a reference for the simulated Subjects.

\subsubsection{Software~Rasterization (CPU)}
The simulation of a DirectCompute-kernel on the CPU using the DirectX~reference~device~driver.
The reference rasterizer supports every Direct3D feature and is intended to be used for debugging- and verification purposes.
Hence, the reference~device~driver is only available on systems where the DirectX~SDK is installed.
As such, although the reference~device~driver does feature some optimization for CPU execution, the driver is not intended for outside-development use~\citeweb[]{drivertypes}.
As the reference~device~driver is designed for the purpose of framework functionality verification, rather than speed, one may expect poor performance.

\subsubsection{Windows Advanced Rasterization Platform (CPU)}
The simulated execution of a DirectCompute kernel using a specially devised software rasterizer devoloped for high performance and full framework conformance by Microsoft in the latest version of DirectX.
Being based on the reference~device~driver, WARP utilizes thread pooling to distribute tasks efficiently on the CPU, along with grouping execution in batches for optimum performance.
Furthermore, WARP uses JIT~(Just-In-Time) compilation to generate SSE2 and SSE4.1 SIMD~(Single Instruction, Multiple Data) instructions, in addition to x86-compatible native instructions to significantly speed up simulation of GPU-bound workloads~\citeweb[]{warp}.
Having outperformed some low-end integrated GPUs according to Microsoft, and the device driver being included in Windows~7 and 8 runtimes, Microsoft recommends using the driver for retail applications, such as casual games~\citeweb[]{warp}.
Although Microsoft has presented some performance comparisons to hardware acceleration, there is no performance benchmark indicative of WARP performance in relation to the reference~device~driver - the device commonly used for debugging and verification purposes.
As such, expectations on the performance of WARP is unclear, but the driver is expected to perform better than the reference~device~driver from which it is derived.\\

These DirectX device drivers make out the Subjects of this study.

% CONTRIBUTION - KERNELS
\subsection{Kernels}
\label{sec:contribution:kernels}
In addition to the Subject drivers mentioned in section~\nameref{sec:contribution:subjectsofstudy}, two kernels with varying level of optimization are examined.
The kernels are implemented in accordance to the CUDA matrix multiplication kernels as described by Kirk~and~Hwu~\cite[p.~67, p.~87]{Kirk:2010:PMP:1841511}.
Furthermore, aforementioned kernels accomodate both integer- and floating point precision.
Descriptions of these kernels is presented below, and are complemented with source code under section~\nameref{sec:appendix}.

\subsubsection{Matrix mult. w. Thread Blocks}
A kernel producing $AB=C$ from two given matrices; writing back $C$ for further analysis.
The program is executed with one thread for each element in the resulting square matrix, and likewise each produce a lone element of the product matrix.
Execution is performed in blocks of $16\times 16$ threads, since this was the block dimension, out of samples $8$, $16$, and $32$, that performed optimally whilst hardware accelerated on the system described under section~\nameref{sec:contribution:equipment}.
This program will be referred to as the Basic~Kernel throughout the material.

\subsubsection{Matrix mult. w. Thread Blocks and Shared Memory}
Similar to the previously described kernel, but optimized to utilize shared memory in order to reduce time-consuming access of global memory, as presented by Kirk~and~Hwu~\cite[p.~77-93]{Kirk:2010:PMP:1841511}.
Stratton~et~al.~\cite[p.~1-3]{Stratton:2008:MEI:1485701.1485703} instructs that the CUDA GPGPU-model may be applied onto multicore CPUs, including locality-wise execution of logical thread blocks (that is; all threads in a block limited to a single core), with the utilization of local- and shared-memory approximately corresponding to the L1-cache of a CPU core.
Hence, the kernel is presented as a scenario due to the preconditions of WARP, stating that a kernel optimized for GPU-execution is likewise optimized for execution with WARP~\citeweb[]{warp}.
Thus, we investigate a more optimized kernel to see whether or not this behaviour may be replicated in the experiment in terms of memory access.
The program will be referred to as the Tiled~Kernel throughout the material.

% CONTRIBUTION - TOOLS
\subsection{Tools}
\label{sec:contribution:tools}
The experiment process is subdivided into three major components, all of which are compiled using Visual~C\texttt{++}~2012.
These programs are presented in detail below.

\subsubsection{matrixgen}
Denotes a utility developed to generate matrices of different dimensions and data-types.
In addition to randomly generating matrices, matrixgen also compiles the reference matrix $Ref$; used to validate the product matrix returned from the subsequent DirectCompute dispatch.
The utility is written in C\texttt{++} and utilizes C\texttt{++}~AMP to generate and multiply matrices $A$~and~$B$ into product matrix $Ref$.
In order to produce random values in a C\texttt{++}~AMP program, the solution includes the random number generator library C\texttt{++}~AMP~RNG.
As matrixgen utilizes Microsoft C\texttt{++}~AMP-technology, the utility requires Windows~7 or later in order to run.

\subsubsection{experiment}
Making out the primary component of the study, experiment uses DirectCompute to compile the product matrix $C$ from matrices $A$~and~$B$ (previously generated by matrixgen).
The application outputs data surrounding the execution of the kernels, such as execution time in milliseconds, to an intermediate file.
The utility is written in C\texttt{++}, with its respective DirectCompute kernels devised in HLSL.
As experiment is developed using the Windows~8~SDK, Windows~8.0 or later is required to run the tool.
Furthermore, experiment requires a DirectX~11.0- or DirectX~11.1-compatible graphics card in order to produce the hardware accelerated reference case.

\subsubsection{analytics}
A utility devised to compose data surrounding possible precisional deviation between matrices $C$~and~$Ref$.
The tool, written in C\texttt{++}, compiles the minimum- and maximum  deviation encountered, as well as to calculate the standard deviation of any precisional discrepency in produced product matrices.
In turn, analytics outputs this information to an intermediate file.\\

These applications are, in turn, run as subprocesses in a script specifying the various configurations and number of times to run each program.
This script, written in Python, compiles the assorted results of respective program and outputs a range of files suitably formatted for interpretation by Gnuplot.
The source code manufactured for the sake of this study is freely available via an online Git repository~\citeweb[]{github}, along with a guide on how to compile and run the solution in order to replicate the experiment.
Furthermore, the complete results presented in this material are also available for download and may be acquired for further analysis.

% Arrowhead command. Sets a tikz node for rendering an entire arrow.
\newcommand\ah[2]{\tikz[remember picture,baseline]\node[inner sep=2pt,outer sep=0](#1){#2};}
% Arrow command. Draws an arrow between two tikz nodes.
\newcommand\arrow[2]{%
\begin{tikzpicture}[remember picture, overlay, >=stealth, shift={(0,0)}]
\draw[->] (#1) to (#2);
\end{tikzpicture}%
}

\restylefloat*{table} % Corrects some weird caption error causing one of the side-by-side captions to be lost (http://tex.stackexchange.com/questions/152626/two-figures-side-by-side-caption-issue).

\begin{table*}
\parbox{.5\linewidth}{
\centering % Consider removing?
\caption{Average execution time in milliseconds\\of a $200\times 200$ integer matrix multiplication and\\corresponding percentage decrease- and increases.}
\label{tab:contribution:results:summaryint}
\begin{tabular}{r|r|r|r|}
	\cline{2-3}
	& \multicolumn{1}{|c|}{\textbf{BASIC}} & \multicolumn{1}{|c|}{\textbf{TILED}} \\ \hline
	\multicolumn{1}{|l|}{\textbf{HARD}}	& \ah{hh1}{$1.16$}	& \ah{hf1}{\phantom{000}$0.24$}\ah{spc}{} 	& \ah{spc}{$-79.3\%$} \\ \hline \hline
	\multicolumn{1}{|l|}{\textbf{SOFT}}	& \ah{vh2}{}\ah{hh2}{$11610.02$}	& \ah{hf2}{$9866.40$}\ah{vh1}{}	& \ah{spc}{$-15.0\%$} \\ \hline
	\multicolumn{1}{|l|}{\textbf{WARP}}	& \ah{vf2}{}\phantom{000}\ah{hh3}{$15.31$}	& \ah{hf3}{\phantom{00}$18.97$}\ah{vf1}{}	& \ah{spc}{$+23.9\%$} \\ \hline
        & \multicolumn{1}{|r|}{\ah{spc}{$-99.9\%$}} & \multicolumn{1}{|r|}{\ah{spc}{$-99.8\%$}} \\ \cline{2-3}
\end{tabular}
\arrow{hh1}{hf1} % Draw horizontal arrow heads to fletchlings.
\arrow{hh2}{hf2}
\arrow{hh3}{hf3}
\arrow{vh1}{vf1} % Draw vertical arrow heads to fletchlings.
\arrow{vh2}{vf2}
}
\hfill
\parbox{.5\linewidth}{
\centering
\caption{Average execution time in milliseconds of\\a $200\times 200$ floating point matrix multiplication and\\ corresponding percentage decrease- and increases.}
\label{tab:contribution:results:summaryfloat}
\begin{tabular}{r|r|r|r|}
	\cline{2-3}
	& \multicolumn{1}{|c|}{\textbf{BASIC}} & \multicolumn{1}{|c|}{\textbf{TILED}} \\ \hline
	\multicolumn{1}{|l|}{\textbf{HARD}}	& \ah{hh1}{$0.77$}	& \ah{hf1}{\phantom{0000}$0.22$}\ah{spc}{}	& \ah{spc}{$-71.4\%$} \\ \hline \hline
	\multicolumn{1}{|l|}{\textbf{SOFT}}	& \ah{vh1}{}\ah{hh2}{$10247.03$}	& \ah{hf2}{$10909.88$}\ah{vh2}{}	& \ah{spc}{$+6.5\%$} \\ \hline
	\multicolumn{1}{|l|}{\textbf{WARP}}	& \ah{vf1}{}\phantom{000}\ah{hh3}{$14.08$}	& \ah{hf3}{\phantom{000}$17.44$}\ah{vf2}{}	& \ah{spc}{$+23.9\%$} \\ \hline
& \multicolumn{1}{|r|}{\ah{spc}{$-99.9\%$}} & \multicolumn{1}{|r|}{\ah{spc}{$-99.8\%$}} \\ \cline{2-3}
\end{tabular}
\arrow{hh1}{hf1} % Draw horizontal arrow heads to fletchlings.
\arrow{hh2}{hf2}
\arrow{hh3}{hf3}
\arrow{vh1}{vf1} % Draw vertical arrow heads to fletchlings.
\arrow{vh2}{vf2}
}
\end{table*}

% CONTRIBUTION - EQUIPMENT
\subsection{Equipment}
\label{sec:contribution:equipment}
The results presented in the paper is gathered from experiments performed on a system with the following specifications:
\begin{description*}
	\item[CPU] Intel Q9550 Quad Core $2.83$GHz
	\item[GPU] ATI Radeon HD 5800
	\item[OS\phantom{U}] Windows~8.0
\end{description*}
This system setup was selected for use as Microsoft claims that the WARP device performs best on modern quad-core CPUs~\citeweb[]{warp}.

% CONTRIBUTION - PROCESS OF STUDY
\subsection{Process of Study}
\label{sec:contribution:processofstudy}
For the purpose of this study, the Object~of~Study, being $200\times 200$ matrices of integer or floating point precision, is randomized with numbers between zero and ten.
The product matrix of these matrices is subsequently computed $100$ times for each configuration.
In this way, the Basic~Kernel is executed with each Subject~of~Study - being Hardware~Acceleration, the reference~device~driver, and WARP - respectively, as is the Tiled~Kernel.
For each execution, data surrounding the dispatch time of each kernel (regardless of program initialization) is garnered along with precision-wise deviational data.
This process is repeated for integer- and floating point precision, in order to establish if there is any performance-wise disrepency betwixt them.

% CONTRIBUTION - RESULTS
\subsection{Results}
\label{sec:contribution:results}
Considering the average percentage decreases in execution time of $99.8$ to $99.9\%$ (presented in table~\ref{tab:contribution:results:summaryint}~and~\ref{tab:contribution:results:summaryfloat}); results indicate significant improvements in the performance of both kernels when using WARP, compared to that of the the reference~device~driver.
These improvements, accounting for three orders of magnitude on average, induce an execution time spanning tenths of milliseconds; a considerable advance from that of roughly ten seconds.
The measurements indicate that the reference~device~driver is $4$~to~$5$ orders of magnitude slower than the hardware accelerated reference case, depending on whether or not the kernel utilizes shared memory to accelerate memory accesses on the video card.
Meanwhile, the WARP device driver performs but $1$~to~$2$ orders of magnitude worse than hardware acceleration, also depending on shared memory utilization.

In terms of memory latency, table~\ref{tab:contribution:results:summaryint} demonstrates performance gains with the hardware accelerated Subject (when utilizing shared memory amongst blocks), with varying results for the other Subjects; either increasing or decreasing execution time (see table~\ref{tab:contribution:results:summaryfloat}).
As such, applicable to both integer- and floating point precision, the performance of WARP is impaired by the kernel utilizing shared memory.
In the Microsoft guide on WARP~\citeweb[]{warp}, the author claims that an application tuned to run efficiently on hardware will run efficiently whilst simulated using WARP as well - and vice versa.
However, the collected data rather indicate a roughly $20\%$ percentual increase in execution time, independant of precision, even though the same kernel accelerates the hardware accelerated Subject by roughly $70$\%.
This demonstrates that that the capabilities of WARP do not extend to memory latency optimization by lower-latency shared memory.
The floating point scenario of this effect is visualized in figure~\ref{fig:contribution:results:warp:msswarp}.

Analysis indicates no loss in precision between reference- and product matrices.
Neither is there any recorded divergence in results compiled by respective Subjects.
In hindsight, this behaviour may be expected as WARP conforms to the precision requirements of the Direct3D~$10$- and $10.1$-specification~\citeweb[]{warp}.
See the Microsoft guide on Floating-point Rules~\citeweb[]{floatingpointrules} for more information surrounding floating point precision in the Direct3D-framework.
Accordingly, albeit not being explicitly developed for the purposes of precision, WARP performs equally as with the reference~device~driver.
Whether or not this behaviour is replicated for double floating point precision is uncertain.

\begin{figure}[htb]
\begin{center}
	\resizebox{ \columnwidth }{!}{\input{../msswarp}}
	\caption{Execution time using WARP with floating point-precision for the Basic and Tiled kernels, indicated by their respective mean- and seperated by standard deviation values. Samples outside of respective standard deviation are not presented.}
	\label{fig:contribution:results:warp:msswarp}
\end{center}
\end{figure}
